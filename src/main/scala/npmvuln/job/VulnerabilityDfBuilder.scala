package npmvuln.job

import org.apache.spark.sql.{DataFrame, Encoders}
import com.github.gundy.semver4j.SemVer
import org.apache.spark.sql.expressions.UserDefinedFunction
import org.apache.spark.sql.functions.{col, udf}
import org.apache.spark.graphx.VertexId
import org.apache.spark.rdd.RDD

import npmvuln.props._

object VulnerabilityDfBuilder {

  // Define version constraint comparation UDF
  val checkConstraint: UserDefinedFunction = udf[Boolean, String, String]((version, constraint) => {
    SemVer.satisfies(version, constraint)
  })

  def build(releasesDf: DataFrame, advisoryDf: DataFrame): RDD[(VertexId, Array[VulnProperties])] = {

    // Join releases dataframe with advisory dataframe where version complies with cnstraint
    releasesDf
      .join(advisoryDf,
        col("Project") === advisoryDf("Package") &&
        this.checkConstraint(col("Release"), advisoryDf("Versions"))
      )

      // Build pairs of vertex ID and vulnerability properties
      .map(row => {
        val vertexId: VertexId = row.getAs[VertexId]("ReleaseId")
        val vulnId: String = row.getAs[String]("Id")
        val vulnName: String = row.getAs[String]("Name")
        val severity: String = row.getAs[String]("Severity")

        (vertexId, new VulnProperties(vulnId, vulnName, severity))
      }) (Encoders.bean(classOf[(VertexId, VulnProperties)]))

      // Get RDD
      .rdd

      // Group by vertex IDs
      .groupByKey

      // Get vulnerability properties inside an array
      .mapValues(_.toArray)
  }

}
