package npmvuln.jobs

import java.sql.Timestamp
import java.time.Instant

import org.apache.spark.sql.{DataFrame, UserDefinedFunction}
import org.apache.spark.sql.functions.{col, udf}
import org.apache.spark.graphx.VertexId
import org.apache.spark.rdd.RDD
import org.threeten.extra.Interval
import com.github.gundy.semver4j.SemVer
import npmvuln.props._

object VulnerabilityDfBuilder {

  // Define version constraint comparation UDF
  val checkConstraint: UserDefinedFunction = udf[Boolean, String, String]((version, constraint) => {
    SemVer.satisfies(version, constraint)
  })

  def build(releasesDf: DataFrame, advisoryDf: DataFrame): RDD[(VertexId, Array[VulnProperties])] = {

    // Join releases dataframe with advisory dataframe where version complies with cnstraint
    releasesDf
      .join(advisoryDf,
        col("Project") === advisoryDf("Package") &&
        this.checkConstraint(col("Release"), advisoryDf("Versions"))
      )

      // Build pairs of vertex ID and vulnerability properties
      .map(row => {
        val vertexId: VertexId = row.getAs[VertexId]("ReleaseId")
        val vulnId: String = row.getAs[String]("Id")
        val vulnName: String = row.getAs[String]("Name")
        val severity: String = row.getAs[String]("Severity")
        val releaseDate: Instant = row.getAs[Timestamp]("Date").toInstant
        val nextReleaseDate: Instant = row.getAs[Timestamp]("NextReleaseDate").toInstant
        val period: Interval = Interval.of(releaseDate, nextReleaseDate)

        (vertexId, new VulnProperties(vulnId, vulnName, severity, period))
      })

      // Group by vertex IDs
      .groupByKey

      // Get vulnerability properties inside an array
      .mapValues(_.toArray)
  }

}
